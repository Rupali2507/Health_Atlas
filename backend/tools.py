"""
Enhanced Provider Validation Tools
===================================

Combines existing tools with advanced VLM/OCR extraction capabilities.

Original Tools:
- NPI Registry Search
- Address Validation (Geoapify)
- Web Scraping (Edge)

Enhanced VLM/OCR:
- Google Gemini Flash 2.0 (Primary) - 95%+ accuracy
- OpenAI GPT-4o-mini (Fallback)
- Anthropic Claude Haiku (Fallback)

Environment Variables Required:
- GOOGLE_API_KEY or GEMINI_API_KEY (for VLM)
- GEOAPIFY_API_KEY (for address validation)
- OPENAI_API_KEY (optional fallback)
- ANTHROPIC_API_KEY (optional fallback)
"""

import requests
import json
import os
import subprocess
import socket
import time
import base64
from io import BytesIO
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

# PDF Processing
from PyPDF2 import PdfReader
from pdf2image import convert_from_path
from PIL import Image

# Web Scraping
from selenium import webdriver
from selenium.webdriver.edge.options import Options
from bs4 import BeautifulSoup

# VLM/OCR APIs
import google.generativeai as genai
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None
    
try:
    from anthropic import Anthropic
except ImportError:
    Anthropic = None

try:
    from pydantic import BaseModel, Field
except ImportError:
    BaseModel = None
    Field = None

from dotenv import load_dotenv

load_dotenv()

# Network configuration for IPv4
def allowed_gai_family():
    return socket.AF_INET

requests.packages.urllib3.util.connection.allowed_gai_family = allowed_gai_family


# ============================================
# ORIGINAL TOOLS (PRESERVED)
# ============================================

# --- TOOL 1: NPI REGISTRY SEARCH ---
def search_npi_registry(
    first_name: str = "",
    last_name: str = "",
    npi_number: str = "",
    state: str = ""
) -> dict:
    """
    Searches the NPPES NPI Registry.
    RULE:
    - If NPI is present ‚Üí query ONLY by NPI (most reliable)
    - Else ‚Üí fallback to name + state search
    """

    print(
        f"\nTOOL: NPPES lookup | "
        f"NPI={npi_number} | Name={first_name} {last_name} | State={state}"
    )

    base_url = "https://npiregistry.cms.hhs.gov/api/"

    # üîí CRITICAL RULE
    if npi_number and npi_number.strip():
        params = {
            "number": npi_number.strip(),
            "version": "2.1"
        }
    else:
        params = {
            "first_name": first_name.strip(),
            "last_name": last_name.strip(),
            "state": state.strip(),
            "version": "2.1"
        }

    try:
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()

        results = data.get("results", [])

        if not results:
            print("TOOL: NPPES ‚Üí NO RESULTS")
            return {
                "match_confidence": 0.0,
                "result_count": 0,
                "results": []
            }

        print(f"TOOL: NPPES ‚Üí {len(results)} result(s) found")

        return {
            "match_confidence": 1.0 if len(results) == 1 else 0.7,
            "result_count": len(results),
            "results": results
        }

    except requests.exceptions.RequestException as e:
        print(f"TOOL: NPPES API ERROR ‚Üí {e}")
        return {
            "match_confidence": 0.0,
            "result_count": 0,
            "error": str(e)
        }


# --- TOOL 2: DYNAMIC WEB SCRAPER (Edge Version) ---
def scrape_provider_website(url: str) -> str:
    """Scrapes text from a website using a headless Microsoft Edge browser."""
    print(f"\nTOOL: Scraping website at URL: {url}")
    
    edge_options = Options()
    edge_options.add_argument("--headless")
    edge_options.add_argument("--no-sandbox")
    edge_options.add_argument("--disable-dev-shm-usage")
    
    driver = None
    try:
        driver = webdriver.Edge(options=edge_options)
        driver.get(url)
        time.sleep(3)
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, "html.parser")
        text_content = soup.get_text(separator=' ', strip=True)
        print("TOOL: Successfully scraped website.")
        return text_content
    except Exception as e:
        return f"An error occurred while scraping the website: {e}"
    finally:
        if driver:
            driver.quit()


# --- TOOL 3: ADDRESS VALIDATION SERVICE (Geoapify Version) ---
def validate_address(address: str, city: str, state: str, zip_code: str) -> dict:
    """Validates an address using the Geoapify Geocoding API."""
    full_address = f"{address}, {city}, {state} {zip_code}, USA"
    print(f"\nTOOL: Validating address with Geoapify: {full_address}")

    api_key = os.environ.get("GEOAPIFY_API_KEY")
    if not api_key:
        return {"error": "GEOAPIFY_API_KEY environment variable not set."}

    url = "https://api.geoapify.com/v1/geocode/search"
    params = {"text": full_address, "apiKey": api_key}

    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()

        if data.get("features"):
            first_result = data["features"][0]["properties"]
            confidence = first_result.get("rank", {}).get("confidence", 0)
            
            verdict = "Not Confident"
            if confidence >= 0.95:
                verdict = "High Confidence Match"
            elif confidence >= 0.7:
                verdict = "Medium Confidence Match"

            result = {
                "verdict": verdict,
                "confidence_score": confidence,
                "found_address": first_result.get("formatted")
            }
            return result
        else:
            return {"verdict": "Address Not Found", "confidence_score": 0}
    except requests.exceptions.RequestException as e:
        return {"error": f"An error occurred calling the Geoapify API: {e}"}


# ============================================
# ENHANCED VLM/OCR EXTRACTION
# ============================================

def extract_with_gemini_flash(pdf_path: str) -> Dict[str, Any]:
    """
    PRIMARY METHOD: Google Gemini Flash - Best free vision model
    
    Advantages:
    - Excellent OCR accuracy (95%+ on medical docs)
    - Free tier: 15 requests/min, 1500/day
    - Native JSON schema support
    - Handles scanned PDFs, handwriting, tables
    """
    
    # Try both GEMINI_API_KEY and GOOGLE_API_KEY for backwards compatibility
    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY or GOOGLE_API_KEY not found in .env file. Get free key at: https://aistudio.google.com/app/apikey")
    
    print(f"üì∏ Extracting with Gemini Flash (Primary VLM)...")
    
    try:
        genai.configure(api_key=api_key)
        
        # First, try to list available models
        model = None
        model_name = None
        
        try:
            print(f"  üîç Detecting available models...")
            available_models = genai.list_models()
            
            # Find first vision-capable model
            for m in available_models:
                if 'generateContent' in m.supported_generation_methods:
                    # Prefer flash models, then pro, then any vision model
                    if 'flash' in m.name.lower() or 'vision' in m.name.lower() or 'pro' in m.name.lower():
                        # Extract just the model name (remove 'models/' prefix)
                        model_name = m.name.split('/')[-1] if '/' in m.name else m.name
                        model = genai.GenerativeModel(model_name)
                        print(f"  ‚úÖ Using model: {model_name}")
                        break
        except Exception as e:
            print(f"  ‚ö†Ô∏è Could not list models: {e}")
        
        # Fallback: Try known model names directly
        if model is None:
            print(f"  üîÑ Trying known model names...")
            model_names = [
                'gemini-1.5-flash',
                'gemini-1.5-pro',
                'gemini-pro-vision',
                'gemini-pro',
                'models/gemini-1.5-flash',
                'models/gemini-pro-vision'
            ]
            
            for test_name in model_names:
                try:
                    model = genai.GenerativeModel(test_name)
                    # Test if model works by doing a simple call
                    model_name = test_name
                    print(f"  ‚úÖ Using model: {model_name}")
                    break
                except Exception as e:
                    continue
        
        if model is None:
            raise ValueError("No compatible Gemini model found. Please check your API key and available models.")
        
        # Convert PDF to images (Gemini works best with images)
        images = convert_from_path(pdf_path, dpi=300, fmt='jpeg')
        print(f"  ‚úÖ Converted PDF to {len(images)} page(s)")
        
        # Process each page
        all_providers = []
        
        for page_num, image in enumerate(images, 1):
            print(f"  üîç Processing page {page_num}/{len(images)}...")
            
            # Create prompt for structured extraction
            prompt = """You are a medical data extraction expert. Extract ALL provider records from this document.

For EACH provider found, extract:
- full_name: Complete name (First Last)
- NPI: 10-digit National Provider Identifier (or empty string if not found)
- specialty: Medical specialty
- address: Street address
- city: City name
- state: 2-letter state code
- zip_code: 5-digit ZIP code
- phone: Phone number (format: XXX-XXX-XXXX)
- license_number: State medical license number
- website: Website URL (if present, otherwise empty string)
- last_updated: Last update date in YYYY-MM-DD format (or empty string)

Return a JSON object with this EXACT structure:
{
  "providers": [
    {
      "full_name": "John Smith",
      "NPI": "1234567890",
      "specialty": "Cardiology",
      "address": "123 Medical Plaza",
      "city": "Los Angeles",
      "state": "CA",
      "zip_code": "90001",
      "phone": "555-123-4567",
      "license_number": "A12345",
      "website": "https://example.com",
      "last_updated": "2024-01-15"
    }
  ],
  "extraction_confidence": 0.95
}

If any field is missing or unclear, use empty string "". Extract ALL providers you can find.
Return ONLY the JSON, no other text."""

            try:
                # Generate response - different approach for different models
                if 'vision' in model._model_name:
                    # Legacy gemini-pro-vision approach
                    response = model.generate_content([prompt, image])
                else:
                    # Modern approach with image bytes
                    img_byte_arr = BytesIO()
                    image.save(img_byte_arr, format='JPEG', quality=95)
                    img_byte_arr = img_byte_arr.getvalue()
                    
                    response = model.generate_content(
                        [prompt, {"mime_type": "image/jpeg", "data": img_byte_arr}],
                        generation_config=genai.GenerationConfig(
                            temperature=0.1  # Low temperature for accuracy
                        )
                    )
                
                response_text = response.text
                
                # Parse JSON response (handle markdown wrappers)
                json_str = response_text.strip()
                json_str = json_str.replace("```json", "").replace("```", "").strip()
                
                page_data = json.loads(json_str)
                page_providers = page_data.get("providers", [])
                
                if page_providers:
                    all_providers.extend(page_providers)
                    print(f"  ‚úÖ Extracted {len(page_providers)} provider(s) from page {page_num}")
                else:
                    print(f"  ‚ö†Ô∏è No providers found on page {page_num}")
                    
            except json.JSONDecodeError as e:
                print(f"  ‚ö†Ô∏è JSON parse error on page {page_num}: {e}")
                print(f"  Response preview: {response_text[:200]}...")
                continue
            except Exception as e:
                print(f"  ‚ö†Ô∏è Error processing page {page_num}: {e}")
                continue
        
        if not all_providers:
            return {
                "error": "No provider data found in PDF",
                "providers": [],
                "extraction_method": "gemini_flash"
            }
        
        print(f"  üéØ Total extracted: {len(all_providers)} provider(s)")
        
        return {
            "providers": all_providers,
            "extraction_confidence": 0.95,  # Gemini Flash is highly accurate
            "extraction_method": "gemini_flash",
            "pages_processed": len(images)
        }
        
    except Exception as e:
        print(f"  ‚ùå Gemini extraction failed: {str(e)}")
        raise


def extract_with_openai_gpt4o_mini(pdf_path: str) -> Dict[str, Any]:
    """
    FALLBACK 1: OpenAI GPT-4o-mini - Excellent vision + cheap
    """
    
    if OpenAI is None:
        raise ImportError("OpenAI library not installed. Install with: pip install openai")
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found")
    
    print(f"ü§ñ Fallback: Extracting with GPT-4o-mini...")
    
    try:
        client = OpenAI(api_key=api_key)
        
        # Convert PDF to images
        images = convert_from_path(pdf_path, dpi=200, fmt='jpeg')
        all_providers = []
        
        for page_num, image in enumerate(images, 1):
            # Convert to base64
            buffered = BytesIO()
            image.save(buffered, format="JPEG", quality=85)
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": """Extract ALL provider records from this medical document. Return JSON with:
{
  "providers": [
    {
      "full_name": "Name",
      "NPI": "10-digit",
      "specialty": "Specialty",
      "address": "Street",
      "city": "City",
      "state": "XX",
      "zip_code": "12345",
      "phone": "XXX-XXX-XXXX",
      "license_number": "License",
      "website": "URL",
      "last_updated": "YYYY-MM-DD"
    }
  ]
}"""
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{img_base64}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            page_data = json.loads(response.choices[0].message.content)
            page_providers = page_data.get("providers", [])
            
            if page_providers:
                all_providers.extend(page_providers)
                print(f"  ‚úÖ Page {page_num}: {len(page_providers)} provider(s)")
        
        return {
            "providers": all_providers,
            "extraction_confidence": 0.90,
            "extraction_method": "gpt4o_mini",
            "pages_processed": len(images)
        }
        
    except Exception as e:
        print(f"  ‚ùå GPT-4o-mini extraction failed: {str(e)}")
        raise


def extract_with_claude_haiku(pdf_path: str) -> Dict[str, Any]:
    """
    FALLBACK 2: Anthropic Claude Haiku - Fast and accurate
    """
    
    if Anthropic is None:
        raise ImportError("Anthropic library not installed. Install with: pip install anthropic")
    
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY not found")
    
    print(f"üåü Fallback: Extracting with Claude Haiku...")
    
    try:
        client = Anthropic(api_key=api_key)
        
        # Convert PDF to images
        images = convert_from_path(pdf_path, dpi=200, fmt='jpeg')
        all_providers = []
        
        for page_num, image in enumerate(images, 1):
            # Convert to base64
            buffered = BytesIO()
            image.save(buffered, format="JPEG", quality=85)
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            
            message = client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=4096,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/jpeg",
                                    "data": img_base64
                                }
                            },
                            {
                                "type": "text",
                                "text": """Extract ALL provider records from this document into JSON:
{
  "providers": [
    {
      "full_name": "Full Name",
      "NPI": "10-digit NPI",
      "specialty": "Medical Specialty",
      "address": "Street Address",
      "city": "City",
      "state": "XX",
      "zip_code": "12345",
      "phone": "XXX-XXX-XXXX",
      "license_number": "License Number",
      "website": "URL",
      "last_updated": "YYYY-MM-DD"
    }
  ]
}

Return ONLY valid JSON, no other text."""
                            }
                        ]
                    }
                ]
            )
            
            # Parse response
            response_text = message.content[0].text
            
            # Extract JSON from response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                page_data = json.loads(json_str)
                page_providers = page_data.get("providers", [])
                
                if page_providers:
                    all_providers.extend(page_providers)
                    print(f"  ‚úÖ Page {page_num}: {len(page_providers)} provider(s)")
        
        return {
            "providers": all_providers,
            "extraction_confidence": 0.88,
            "extraction_method": "claude_haiku",
            "pages_processed": len(images)
        }
        
    except Exception as e:
        print(f"  ‚ùå Claude Haiku extraction failed: {str(e)}")
        raise


# ============================================
# MAIN PDF PARSING FUNCTION (ENHANCED)
# ============================================

def parse_provider_pdf(pdf_path: str, force_method: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Enhanced multi-model provider extraction with automatic fallbacks.
    
    This is the MAIN function used by your application.
    It replaces the old parse_provider_pdf function with a more robust version.
    
    Args:
        pdf_path: Path to PDF or image file
        force_method: Force specific method ("gemini", "openai", "claude")
    
    Returns:
        List of provider dictionaries
    """
    
    print(f"\n{'='*60}")
    print(f"üè• PROVIDER DATA EXTRACTION")
    print(f"{'='*60}")
    print(f"File: {pdf_path}")
    print(f"{'='*60}\n")
    
    if not os.path.exists(pdf_path):
        return [{"error": f"File not found: {pdf_path}"}]
    
    # Define extraction pipeline with fallbacks
    extraction_methods = [
        ("gemini", extract_with_gemini_flash),
        ("openai", extract_with_openai_gpt4o_mini),
        ("claude", extract_with_claude_haiku)
    ]
    
    # If force_method specified, try only that one
    if force_method:
        extraction_methods = [(m, f) for m, f in extraction_methods if m == force_method]
    
    last_error = None
    
    for method_name, extraction_func in extraction_methods:
        try:
            # Check if API key exists
            key_map = {
                "gemini": ["GEMINI_API_KEY", "GOOGLE_API_KEY"],  # Support both keys
                "openai": ["OPENAI_API_KEY"],
                "claude": ["ANTHROPIC_API_KEY"]
            }
            
            # Check if any required key exists
            has_key = any(os.getenv(k) for k in key_map[method_name])
            
            if not has_key:
                print(f"‚è≠Ô∏è Skipping {method_name.upper()}: API key not found")
                continue
            
            # Attempt extraction
            result = extraction_func(pdf_path)
            
            providers = result.get("providers", [])
            
            if not providers:
                print(f"‚ö†Ô∏è {method_name.upper()}: No providers extracted, trying next method...")
                continue
            
            # Validate and enrich data
            validated_providers = []
            for provider in providers:
                # Ensure all required fields exist
                validated_provider = {
                    "full_name": provider.get("full_name") or provider.get("fullName", ""),
                    "NPI": provider.get("NPI") or provider.get("npi", ""),
                    "specialty": provider.get("specialty", ""),
                    "address": provider.get("address", ""),
                    "city": provider.get("city", ""),
                    "state": provider.get("state", ""),
                    "zip_code": provider.get("zip_code") or provider.get("zipCode", ""),
                    "phone": provider.get("phone", ""),
                    "license_number": provider.get("license_number") or provider.get("license", ""),
                    "website": provider.get("website", ""),
                    "last_updated": provider.get("last_updated") or provider.get("lastUpdated", datetime.now().strftime("%Y-%m-%d"))
                }
                
                # Only include if minimum required fields are present
                if validated_provider["full_name"]:  # At minimum need a name
                    validated_providers.append(validated_provider)
            
            if not validated_providers:
                print(f"‚ö†Ô∏è {method_name.upper()}: No valid providers after validation")
                continue
            
            # SUCCESS!
            print(f"\n{'='*60}")
            print(f"‚úÖ EXTRACTION SUCCESSFUL")
            print(f"{'='*60}")
            print(f"Method: {result.get('extraction_method', method_name).upper()}")
            print(f"Providers: {len(validated_providers)}")
            print(f"Confidence: {result.get('extraction_confidence', 0)*100:.1f}%")
            print(f"Pages: {result.get('pages_processed', 'N/A')}")
            print(f"{'='*60}\n")
            
            return validated_providers
            
        except Exception as e:
            last_error = str(e)
            print(f"‚ùå {method_name.upper()} failed: {last_error}")
            print(f"   Attempting next method...\n")
            continue
    
    # All methods failed
    error_msg = f"All extraction methods failed. Last error: {last_error}"
    print(f"\n{'='*60}")
    print(f"‚ùå EXTRACTION FAILED")
    print(f"{'='*60}")
    print(error_msg)
    print(f"{'='*60}\n")
    
    return [{"error": error_msg}]


# ============================================
# BACKWARD COMPATIBILITY (OLD FUNCTIONS)
# ============================================

def parse_provider_pdf_simple(pdf_path: str) -> str:
    """
    LEGACY FUNCTION: Simple text extraction using PyPDF2.
    Kept for backward compatibility but not recommended.
    Use parse_provider_pdf() instead for better results.
    """
    print(f"\nTOOL: Parsing PDF '{pdf_path}' using PyPDF2 (Legacy Mode)...")
    if not os.path.exists(pdf_path):
        return f"Error: PDF not found at '{pdf_path}'"
    try:
        reader = PdfReader(pdf_path)
        extracted_text = ""
        for page in reader.pages:
            extracted_text += page.extract_text() + "\n"
        print("TOOL: Successfully extracted text from PDF.")
        return extracted_text
    except Exception as e:
        return f"An unexpected error occurred while parsing the PDF: {e}"


# ============================================
# TEST CASES
# ============================================

if __name__ == '__main__':
    load_dotenv()
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

    print("="*60)
    print("RUNNING ALL TOOL TESTS")
    print("="*60)
    
    print("\n--- Test 1: Search by NPI ---")
    print(json.dumps(search_npi_registry(npi_number="1235256050"), indent=2))
    
    print("\n--- Test 2: Parse Provider PDF (Enhanced VLM) ---")
    pdf_file_path = os.path.join(SCRIPT_DIR, 'Sample_Data', 'provider_directory.pdf')
    if os.path.exists(pdf_file_path):
        providers = parse_provider_pdf(pdf_path=pdf_file_path)
        if providers and not providers[0].get("error"):
            print(f"\n‚úÖ Successfully extracted {len(providers)} provider(s)")
            print(f"\nFirst provider:")
            print(json.dumps(providers[0], indent=2))
        else:
            print(f"\n‚ùå Extraction failed: {providers[0].get('error')}")
    else:
        print(f"‚ö†Ô∏è Sample PDF not found at: {pdf_file_path}")
    
    print("\n--- Test 3: Scrape Provider Website ---")
    test_url = "https://my.clevelandclinic.org/staff/9953-robert-ackerman"
    scraped_text = scrape_provider_website(test_url)
    print(f"\nTest Result (first 500 chars):\n{scraped_text[:500]}")

    print("\n--- Test 4: Validate Address (Geoapify) ---")
    validation_result_good = validate_address(
        address="1600 Amphitheatre Parkway", 
        city="Mountain View", 
        state="CA", 
        zip_code="94043"
    )
    print(json.dumps(validation_result_good, indent=2))
    
    print("\n" + "="*60)
    print("ALL TESTS COMPLETED")
    print("="*60)