"""
Enhanced Provider Validation Tools - UNIVERSAL FORMAT SUPPORT
==============================================================

Supports ALL file formats:
‚úÖ Typed PDFs (text-based)
‚úÖ Scanned PDFs (image-based, OCR required)
‚úÖ Images (PNG, JPG, JPEG, TIFF, BMP, WebP)
‚úÖ Excel files (XLSX, XLS, XLSM, XLSB)
‚úÖ CSV/TSV files
‚úÖ Mixed format batches

Original Tools (Preserved):
- NPI Registry Search
- Address Validation (Geoapify)
- Web Scraping (Edge)

Enhanced VLM/OCR Pipeline:
- Google Gemini Flash 2.0 (Primary) - 95%+ accuracy
- OpenAI GPT-4o-mini (Fallback)
- Anthropic Claude Haiku (Fallback)

Environment Variables Required:
- GOOGLE_API_KEY or GEMINI_API_KEY (for VLM) - GET FREE: https://aistudio.google.com/app/apikey
- GEOAPIFY_API_KEY (for address validation)
- OPENAI_API_KEY (optional fallback)
- ANTHROPIC_API_KEY (optional fallback)
"""

import requests
import json
import os
import subprocess
import socket
import time
import base64
from io import BytesIO
from typing import List, Dict, Any, Optional, Union, Tuple
from datetime import datetime
from pathlib import Path
import mimetypes

# PDF Processing
from PyPDF2 import PdfReader
from pdf2image import convert_from_path
from PIL import Image
import pandas as pd

# Web Scraping
from selenium import webdriver
from selenium.webdriver.edge.options import Options
from bs4 import BeautifulSoup

# VLM/OCR APIs
import google.generativeai as genai
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None
    
try:
    from anthropic import Anthropic
except ImportError:
    Anthropic = None

from dotenv import load_dotenv

load_dotenv()

# Network configuration for IPv4
def allowed_gai_family():
    return socket.AF_INET

requests.packages.urllib3.util.connection.allowed_gai_family = allowed_gai_family


# ============================================
# FILE TYPE DETECTION
# ============================================

def detect_file_type(file_path: str) -> Tuple[str, str]:
    """
    Detect file type and category.
    
    Returns:
        (category, specific_type) where:
        - category: 'image', 'pdf', 'excel', 'csv', 'unknown'
        - specific_type: exact MIME type or extension
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")
    
    # Get extension
    extension = file_path.suffix.lower()
    
    # Detect by extension first (most reliable)
    image_extensions = {'.png', '.jpg', '.jpeg', '.tiff', '.tif', '.bmp', '.webp', '.gif'}
    pdf_extensions = {'.pdf'}
    excel_extensions = {'.xlsx', '.xls', '.xlsm', '.xlsb'}
    csv_extensions = {'.csv', '.tsv', '.txt'}
    
    if extension in image_extensions:
        return ('image', extension[1:])
    elif extension in pdf_extensions:
        # Check if it's scanned or typed
        try:
            reader = PdfReader(str(file_path))
            if len(reader.pages) > 0:
                text = reader.pages[0].extract_text().strip()
                if len(text) > 50:  # Has extractable text
                    return ('pdf', 'typed')
                else:
                    return ('pdf', 'scanned')
        except:
            return ('pdf', 'scanned')
    elif extension in excel_extensions:
        return ('excel', extension[1:])
    elif extension in csv_extensions:
        return ('csv', extension[1:])
    else:
        # Fallback to MIME type
        mime_type, _ = mimetypes.guess_type(str(file_path))
        if mime_type:
            if mime_type.startswith('image/'):
                return ('image', mime_type.split('/')[1])
            elif mime_type == 'application/pdf':
                return ('pdf', 'unknown')
        
        return ('unknown', extension[1:] if extension else 'unknown')


# ============================================
# ORIGINAL TOOLS (PRESERVED)
# ============================================

def search_npi_registry(
    first_name: str = "",
    last_name: str = "",
    npi_number: str = "",
    state: str = ""
) -> dict:
    """
    Searches the NPPES NPI Registry.
    RULE:
    - If NPI is present ‚Üí query ONLY by NPI (most reliable)
    - Else ‚Üí fallback to name + state search
    """

    print(
        f"\nTOOL: NPPES lookup | "
        f"NPI={npi_number} | Name={first_name} {last_name} | State={state}"
    )

    base_url = "https://npiregistry.cms.hhs.gov/api/"

    # üîí CRITICAL RULE
    if npi_number and npi_number.strip():
        params = {
            "number": npi_number.strip(),
            "version": "2.1"
        }
    else:
        params = {
            "first_name": first_name.strip(),
            "last_name": last_name.strip(),
            "state": state.strip(),
            "version": "2.1"
        }

    try:
        response = requests.get(base_url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()

        results = data.get("results", [])

        if not results:
            print("TOOL: NPPES ‚Üí NO RESULTS")
            return {
                "match_confidence": 0.0,
                "result_count": 0,
                "results": []
            }

        print(f"TOOL: NPPES ‚Üí {len(results)} result(s) found")

        return {
            "match_confidence": 1.0 if len(results) == 1 else 0.7,
            "result_count": len(results),
            "results": results
        }

    except requests.exceptions.RequestException as e:
        print(f"TOOL: NPPES API ERROR ‚Üí {e}")
        return {
            "match_confidence": 0.0,
            "result_count": 0,
            "error": str(e)
        }


def scrape_provider_website(url: str) -> str:
    """Scrapes text from a website using a headless Microsoft Edge browser."""
    print(f"\nTOOL: Scraping website at URL: {url}")
    
    edge_options = Options()
    edge_options.add_argument("--headless")
    edge_options.add_argument("--no-sandbox")
    edge_options.add_argument("--disable-dev-shm-usage")
    
    driver = None
    try:
        driver = webdriver.Edge(options=edge_options)
        driver.get(url)
        time.sleep(3)
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, "html.parser")
        text_content = soup.get_text(separator=' ', strip=True)
        print("TOOL: Successfully scraped website.")
        return text_content
    except Exception as e:
        return f"An error occurred while scraping the website: {e}"
    finally:
        if driver:
            driver.quit()


def validate_address(address: str, city: str, state: str, zip_code: str) -> dict:
    """Validates an address using the Geoapify Geocoding API."""
    full_address = f"{address}, {city}, {state} {zip_code}, USA"
    print(f"\nTOOL: Validating address with Geoapify: {full_address}")

    api_key = os.environ.get("GEOAPIFY_API_KEY")
    if not api_key:
        return {"error": "GEOAPIFY_API_KEY environment variable not set."}

    url = "https://api.geoapify.com/v1/geocode/search"
    params = {"text": full_address, "apiKey": api_key}

    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()

        if data.get("features"):
            first_result = data["features"][0]["properties"]
            confidence = first_result.get("rank", {}).get("confidence", 0)
            
            verdict = "Not Confident"
            if confidence >= 0.95:
                verdict = "High Confidence Match"
            elif confidence >= 0.7:
                verdict = "Medium Confidence Match"

            result = {
                "verdict": verdict,
                "confidence_score": confidence,
                "found_address": first_result.get("formatted")
            }
            return result
        else:
            return {"verdict": "Address Not Found", "confidence_score": 0}
    except requests.exceptions.RequestException as e:
        return {"error": f"An error occurred calling the Geoapify API: {e}"}


# ============================================
# EXCEL & CSV PARSERS
# ============================================

def parse_excel_file(file_path: str) -> List[Dict[str, Any]]:
    """
    Parse Excel files (XLSX, XLS, XLSM, XLSB) to extract provider data.
    
    Handles:
    - Multiple sheets (searches all sheets)
    - Various header formats
    - Merged cells
    - Empty rows/columns
    """
    print(f"\nüìä Parsing Excel file: {file_path}")
    
    try:
        # Try to read all sheets
        all_sheets = pd.read_excel(file_path, sheet_name=None, engine='openpyxl')
        
        all_providers = []
        
        for sheet_name, df in all_sheets.items():
            print(f"  üìÑ Processing sheet: {sheet_name}")
            
            # Clean column names (lowercase, strip spaces)
            df.columns = df.columns.str.strip().str.lower()
            
            # Skip empty sheets
            if df.empty:
                print(f"    ‚ö†Ô∏è Sheet is empty, skipping")
                continue
            
            # Flexible column mapping (handle various naming conventions)
            column_map = {
                'full_name': ['full_name', 'fullname', 'name', 'provider_name', 'provider', 'doctor', 'physician'],
                'NPI': ['npi', 'npi_number', 'npi#', 'national_provider_identifier'],
                'specialty': ['specialty', 'speciality', 'type', 'provider_type', 'medical_specialty'],
                'address': ['address', 'street', 'street_address', 'address_1', 'address1'],
                'city': ['city', 'town'],
                'state': ['state', 'st'],
                'zip_code': ['zip', 'zip_code', 'zipcode', 'postal_code', 'zip code'],
                'phone': ['phone', 'telephone', 'phone_number', 'tel', 'contact'],
                'license_number': ['license', 'license_number', 'license#', 'medical_license', 'lic'],
                'website': ['website', 'url', 'web', 'site'],
                'last_updated': ['last_updated', 'updated', 'date', 'last_update', 'update_date']
            }
            
            # Find matching columns
            field_columns = {}
            for field, possible_names in column_map.items():
                for col in df.columns:
                    if any(pn in col for pn in possible_names):
                        field_columns[field] = col
                        break
            
            print(f"    ‚úÖ Mapped {len(field_columns)} fields: {list(field_columns.keys())}")
            
            # Extract providers row by row
            for idx, row in df.iterrows():
                provider = {}
                
                for field, column in field_columns.items():
                    value = row[column]
                    
                    # Handle NaN/None values
                    if pd.isna(value):
                        provider[field] = ""
                    else:
                        # Convert to string and clean
                        provider[field] = str(value).strip()
                
                # Only add if we have at least a name
                if provider.get('full_name'):
                    # Fill missing fields
                    for field in column_map.keys():
                        if field not in provider:
                            provider[field] = ""
                    
                    all_providers.append(provider)
            
            print(f"    ‚úÖ Extracted {len(all_providers)} provider(s) from this sheet")
        
        print(f"  üéØ Total: {len(all_providers)} provider(s) from {len(all_sheets)} sheet(s)")
        return all_providers
        
    except Exception as e:
        print(f"  ‚ùå Excel parsing failed: {str(e)}")
        return [{"error": f"Excel parsing failed: {str(e)}"}]


def parse_csv_file(file_path: str) -> List[Dict[str, Any]]:
    """
    Parse CSV/TSV files to extract provider data.
    
    Handles:
    - Various delimiters (auto-detected)
    - Different encodings
    - Headers in different formats
    """
    print(f"\nüìã Parsing CSV file: {file_path}")
    
    try:
        # Try different encodings
        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
        df = None
        
        for encoding in encodings:
            try:
                df = pd.read_csv(file_path, encoding=encoding)
                print(f"  ‚úÖ Successfully read with {encoding} encoding")
                break
            except UnicodeDecodeError:
                continue
        
        if df is None:
            raise ValueError("Could not read CSV with any standard encoding")
        
        # Clean column names
        df.columns = df.columns.str.strip().str.lower()
        
        # Use same column mapping as Excel
        column_map = {
            'full_name': ['full_name', 'fullname', 'name', 'provider_name', 'provider', 'doctor', 'physician'],
            'NPI': ['npi', 'npi_number', 'npi#', 'national_provider_identifier'],
            'specialty': ['specialty', 'speciality', 'type', 'provider_type', 'medical_specialty'],
            'address': ['address', 'street', 'street_address', 'address_1', 'address1'],
            'city': ['city', 'town'],
            'state': ['state', 'st'],
            'zip_code': ['zip', 'zip_code', 'zipcode', 'postal_code', 'zip code'],
            'phone': ['phone', 'telephone', 'phone_number', 'tel', 'contact'],
            'license_number': ['license', 'license_number', 'license#', 'medical_license', 'lic'],
            'website': ['website', 'url', 'web', 'site'],
            'last_updated': ['last_updated', 'updated', 'date', 'last_update', 'update_date']
        }
        
        # Find matching columns
        field_columns = {}
        for field, possible_names in column_map.items():
            for col in df.columns:
                if any(pn in col for pn in possible_names):
                    field_columns[field] = col
                    break
        
        print(f"  ‚úÖ Mapped {len(field_columns)} fields: {list(field_columns.keys())}")
        
        # Extract providers
        providers = []
        for idx, row in df.iterrows():
            provider = {}
            
            for field, column in field_columns.items():
                value = row[column]
                
                if pd.isna(value):
                    provider[field] = ""
                else:
                    provider[field] = str(value).strip()
            
            # Only add if we have at least a name
            if provider.get('full_name'):
                # Fill missing fields
                for field in column_map.keys():
                    if field not in provider:
                        provider[field] = ""
                
                providers.append(provider)
        
        print(f"  üéØ Extracted {len(providers)} provider(s)")
        return providers
        
    except Exception as e:
        print(f"  ‚ùå CSV parsing failed: {str(e)}")
        return [{"error": f"CSV parsing failed: {str(e)}"}]


# ============================================
# IMAGE PREPROCESSING
# ============================================

def preprocess_image(image: Image.Image, enhance: bool = True) -> Image.Image:
    """
    Preprocess image for better OCR/VLM results.
    
    Enhancements:
    - Resize if too large/small
    - Convert to RGB
    - Optional: sharpen, contrast adjustment
    """
    # Convert to RGB if needed
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # Resize if needed (optimal: 1500-3000px on longest side)
    max_dim = max(image.size)
    if max_dim > 3000:
        scale = 3000 / max_dim
        new_size = (int(image.size[0] * scale), int(image.size[1] * scale))
        image = image.resize(new_size, Image.Resampling.LANCZOS)
        print(f"    üìê Resized to {new_size}")
    elif max_dim < 1000:
        scale = 1500 / max_dim
        new_size = (int(image.size[0] * scale), int(image.size[1] * scale))
        image = image.resize(new_size, Image.Resampling.LANCZOS)
        print(f"    üìê Upscaled to {new_size}")
    
    # Optional enhancements
    if enhance:
        from PIL import ImageEnhance
        
        # Increase sharpness slightly
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.2)
        
        # Increase contrast slightly
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.1)
    
    return image


# ============================================
# ENHANCED VLM/OCR EXTRACTION
# ============================================

def extract_with_gemini_flash(images: List[Image.Image], source_name: str = "document") -> Dict[str, Any]:
    """
    PRIMARY METHOD: Google Gemini Flash - Best free vision model
    
    Now accepts pre-loaded PIL Images instead of PDF path
    """
    
    # Try both GEMINI_API_KEY and GOOGLE_API_KEY
    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY or GOOGLE_API_KEY not found in .env file. Get free key at: https://aistudio.google.com/app/apikey")
    
    print(f"üì∏ Extracting with Gemini Flash (Primary VLM)...")
    
    try:
        genai.configure(api_key=api_key)
        
        # Find available model
        model = None
        model_name = None
        
        try:
            print(f"  üîç Detecting available models...")
            available_models = genai.list_models()
            
            for m in available_models:
                if 'generateContent' in m.supported_generation_methods:
                    if 'flash' in m.name.lower() or 'vision' in m.name.lower() or 'pro' in m.name.lower():
                        model_name = m.name.split('/')[-1] if '/' in m.name else m.name
                        model = genai.GenerativeModel(model_name)
                        print(f"  ‚úÖ Using model: {model_name}")
                        break
        except Exception as e:
            print(f"  ‚ö†Ô∏è Could not list models: {e}")
        
        # Fallback to known model names
        if model is None:
            print(f"  üîÑ Trying known model names...")
            model_names = [
                'gemini-1.5-flash',
                'gemini-1.5-pro',
                'gemini-pro-vision',
                'gemini-pro',
                'models/gemini-1.5-flash',
                'models/gemini-pro-vision'
            ]
            
            for test_name in model_names:
                try:
                    model = genai.GenerativeModel(test_name)
                    model_name = test_name
                    print(f"  ‚úÖ Using model: {model_name}")
                    break
                except:
                    continue
        
        if model is None:
            raise ValueError("No compatible Gemini model found")
        
        # Create extraction prompt
        prompt = """You are a medical data extraction expert. Extract ALL provider records from this document.

For EACH provider found, extract:
- full_name: Complete name (First Last)
- NPI: 10-digit National Provider Identifier (or empty string if not found)
- specialty: Medical specialty
- address: Street address
- city: City name
- state: 2-letter state code
- zip_code: 5-digit ZIP code
- phone: Phone number (format: XXX-XXX-XXXX)
- license_number: State medical license number
- website: Website URL (if present, otherwise empty string)
- last_updated: Last update date in YYYY-MM-DD format (or empty string)

Return a JSON object with this EXACT structure:
{
  "providers": [
    {
      "full_name": "John Smith",
      "NPI": "1234567890",
      "specialty": "Cardiology",
      "address": "123 Medical Plaza",
      "city": "Los Angeles",
      "state": "CA",
      "zip_code": "90001",
      "phone": "555-123-4567",
      "license_number": "A12345",
      "website": "https://example.com",
      "last_updated": "2024-01-15"
    }
  ],
  "extraction_confidence": 0.95
}

If any field is missing or unclear, use empty string "". Extract ALL providers you can find.
Return ONLY the JSON, no other text."""

        # Process each page
        all_providers = []
        
        for page_num, image in enumerate(images, 1):
            print(f"  üîç Processing page {page_num}/{len(images)}...")
            
            # Preprocess image
            image = preprocess_image(image)
            
            try:
                # Convert to bytes
                img_byte_arr = BytesIO()
                image.save(img_byte_arr, format='JPEG', quality=95)
                img_byte_arr = img_byte_arr.getvalue()
                
                # Generate response
                response = model.generate_content(
                    [prompt, {"mime_type": "image/jpeg", "data": img_byte_arr}],
                    generation_config=genai.GenerationConfig(temperature=0.1)
                )
                
                response_text = response.text
                
                # Parse JSON
                json_str = response_text.strip()
                json_str = json_str.replace("```json", "").replace("```", "").strip()
                
                page_data = json.loads(json_str)
                page_providers = page_data.get("providers", [])
                
                if page_providers:
                    all_providers.extend(page_providers)
                    print(f"  ‚úÖ Extracted {len(page_providers)} provider(s) from page {page_num}")
                else:
                    print(f"  ‚ö†Ô∏è No providers found on page {page_num}")
                    
            except json.JSONDecodeError as e:
                print(f"  ‚ö†Ô∏è JSON parse error on page {page_num}: {e}")
                continue
            except Exception as e:
                print(f"  ‚ö†Ô∏è Error processing page {page_num}: {e}")
                continue
        
        if not all_providers:
            return {
                "error": f"No provider data found in {source_name}",
                "providers": [],
                "extraction_method": "gemini_flash"
            }
        
        print(f"  üéØ Total extracted: {len(all_providers)} provider(s)")
        
        return {
            "providers": all_providers,
            "extraction_confidence": 0.95,
            "extraction_method": "gemini_flash",
            "pages_processed": len(images)
        }
        
    except Exception as e:
        print(f"  ‚ùå Gemini extraction failed: {str(e)}")
        raise


def extract_with_openai_gpt4o_mini(images: List[Image.Image], source_name: str = "document") -> Dict[str, Any]:
    """FALLBACK 1: OpenAI GPT-4o-mini"""
    
    if OpenAI is None:
        raise ImportError("OpenAI library not installed")
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found")
    
    print(f"ü§ñ Fallback: Extracting with GPT-4o-mini...")
    
    try:
        client = OpenAI(api_key=api_key)
        all_providers = []
        
        for page_num, image in enumerate(images, 1):
            # Preprocess
            image = preprocess_image(image)
            
            # Convert to base64
            buffered = BytesIO()
            image.save(buffered, format="JPEG", quality=85)
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": """Extract ALL provider records from this medical document. Return JSON with:
{
  "providers": [
    {
      "full_name": "Name",
      "NPI": "10-digit",
      "specialty": "Specialty",
      "address": "Street",
      "city": "City",
      "state": "XX",
      "zip_code": "12345",
      "phone": "XXX-XXX-XXXX",
      "license_number": "License",
      "website": "URL",
      "last_updated": "YYYY-MM-DD"
    }
  ]
}"""
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{img_base64}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.1
            )
            
            page_data = json.loads(response.choices[0].message.content)
            page_providers = page_data.get("providers", [])
            
            if page_providers:
                all_providers.extend(page_providers)
                print(f"  ‚úÖ Page {page_num}: {len(page_providers)} provider(s)")
        
        return {
            "providers": all_providers,
            "extraction_confidence": 0.90,
            "extraction_method": "gpt4o_mini",
            "pages_processed": len(images)
        }
        
    except Exception as e:
        print(f"  ‚ùå GPT-4o-mini extraction failed: {str(e)}")
        raise


def extract_with_claude_haiku(images: List[Image.Image], source_name: str = "document") -> Dict[str, Any]:
    """FALLBACK 2: Anthropic Claude Haiku"""
    
    if Anthropic is None:
        raise ImportError("Anthropic library not installed")
    
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        raise ValueError("ANTHROPIC_API_KEY not found")
    
    print(f"üåü Fallback: Extracting with Claude Haiku...")
    
    try:
        client = Anthropic(api_key=api_key)
        all_providers = []
        
        for page_num, image in enumerate(images, 1):
            # Preprocess
            image = preprocess_image(image)
            
            # Convert to base64
            buffered = BytesIO()
            image.save(buffered, format="JPEG", quality=85)
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            
            message = client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=4096,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image",
                                "source": {
                                    "type": "base64",
                                    "media_type": "image/jpeg",
                                    "data": img_base64
                                }
                            },
                            {
                                "type": "text",
                                "text": """Extract ALL provider records from this document into JSON:
{
  "providers": [
    {
      "full_name": "Full Name",
      "NPI": "10-digit NPI",
      "specialty": "Medical Specialty",
      "address": "Street Address",
      "city": "City",
      "state": "XX",
      "zip_code": "12345",
      "phone": "XXX-XXX-XXXX",
      "license_number": "License Number",
      "website": "URL",
      "last_updated": "YYYY-MM-DD"
    }
  ]
}

Return ONLY valid JSON, no other text."""
                            }
                        ]
                    }
                ]
            )
            
            response_text = message.content[0].text
            
            # Extract JSON
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response_text[json_start:json_end]
                page_data = json.loads(json_str)
                page_providers = page_data.get("providers", [])
                
                if page_providers:
                    all_providers.extend(page_providers)
                    print(f"  ‚úÖ Page {page_num}: {len(page_providers)} provider(s)")
        
        return {
            "providers": all_providers,
            "extraction_confidence": 0.88,
            "extraction_method": "claude_haiku",
            "pages_processed": len(images)
        }
        
    except Exception as e:
        print(f"  ‚ùå Claude Haiku extraction failed: {str(e)}")
        raise


# ============================================
# UNIVERSAL FILE LOADER
# ============================================

def load_file_as_images(file_path: str) -> Tuple[List[Image.Image], str]:
    """
    Load ANY file format and convert to images for VLM processing.
    
    Supports:
    - PDF (typed or scanned)
    - Images (PNG, JPG, TIFF, etc.)
    - Excel (converts each sheet to image)
    - CSV (converts to image)
    
    Returns:
        (list of PIL Images, source_description)
    """
    category, specific_type = detect_file_type(file_path)
    
    print(f"\nüìÇ Loading file: {file_path}")
    print(f"   Type: {category} ({specific_type})")
    
    images = []
    source_desc = f"{category} file"
    
    if category == 'pdf':
        # Convert PDF to images
        print(f"   üîÑ Converting PDF to images (DPI: 300)...")
        images = convert_from_path(file_path, dpi=300, fmt='jpeg')
        print(f"   ‚úÖ Generated {len(images)} page image(s)")
        source_desc = f"PDF ({specific_type})"
        
    elif category == 'image':
        # Load image directly
        print(f"   üì∑ Loading image...")
        img = Image.open(file_path)
        images = [img]
        print(f"   ‚úÖ Loaded {img.size[0]}x{img.size[1]} image")
        source_desc = f"{specific_type.upper()} image"
        
    elif category == 'excel':
        # Convert Excel sheets to images (for VLM fallback)
        print(f"   üìä Excel file detected - will try structured parsing first")
        # Note: We'll handle Excel separately, but provide VLM fallback
        source_desc = f"Excel ({specific_type})"
        
    elif category == 'csv':
        # Convert CSV to image (for VLM fallback)
        print(f"   üìã CSV file detected - will try structured parsing first")
        source_desc = f"CSV ({specific_type})"
        
    else:
        raise ValueError(f"Unsupported file type: {category} ({specific_type})")
    
    return images, source_desc


# ============================================
# MAIN UNIVERSAL PARSER
# ============================================

def parse_provider_file(file_path: str, force_method: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    üéØ MAIN FUNCTION - Universal Provider Data Extraction
    
    Supports ALL formats:
    ‚úÖ Typed PDFs
    ‚úÖ Scanned PDFs
    ‚úÖ Images (PNG, JPG, TIFF, etc.)
    ‚úÖ Excel (XLSX, XLS, XLSM, XLSB)
    ‚úÖ CSV/TSV
    
    Strategy:
    1. Detect file type
    2. For structured formats (Excel, CSV): Use pandas parsing
    3. For visual formats (PDF, images): Use VLM extraction
    4. Automatic fallback between methods
    
    Args:
        file_path: Path to any supported file
        force_method: Force specific VLM ("gemini", "openai", "claude")
    
    Returns:
        List of provider dictionaries with standardized fields
    """
    
    print(f"\n{'='*60}")
    print(f"üè• UNIVERSAL PROVIDER DATA EXTRACTION")
    print(f"{'='*60}")
    print(f"File: {file_path}")
    print(f"{'='*60}\n")
    
    if not os.path.exists(file_path):
        return [{"error": f"File not found: {file_path}"}]
    
    # Detect file type
    try:
        category, specific_type = detect_file_type(file_path)
    except Exception as e:
        return [{"error": f"Could not detect file type: {str(e)}"}]
    
    print(f"üìã File Category: {category.upper()}")
    print(f"üìã Specific Type: {specific_type}")
    print(f"{'='*60}\n")
    
    # STRATEGY 1: Structured file formats (Excel, CSV)
    if category == 'excel':
        print("üîπ Using STRUCTURED PARSING (pandas + openpyxl)")
        providers = parse_excel_file(file_path)
        
        # Check if successful
        if providers and not providers[0].get("error"):
            print(f"\n{'='*60}")
            print(f"‚úÖ EXCEL PARSING SUCCESSFUL")
            print(f"{'='*60}")
            print(f"Providers: {len(providers)}")
            print(f"Method: Structured Excel Parsing")
            print(f"{'='*60}\n")
            return providers
        else:
            print("‚ö†Ô∏è Excel parsing failed, falling back to VLM extraction...")
    
    elif category == 'csv':
        print("üîπ Using STRUCTURED PARSING (pandas CSV reader)")
        providers = parse_csv_file(file_path)
        
        if providers and not providers[0].get("error"):
            print(f"\n{'='*60}")
            print(f"‚úÖ CSV PARSING SUCCESSFUL")
            print(f"{'='*60}")
            print(f"Providers: {len(providers)}")
            print(f"Method: Structured CSV Parsing")
            print(f"{'='*60}\n")
            return providers
        else:
            print("‚ö†Ô∏è CSV parsing failed, falling back to VLM extraction...")
    
    # STRATEGY 2: Visual extraction (PDF, images, or fallback from Excel/CSV)
    print(f"üîπ Using VISUAL EXTRACTION (VLM/OCR)")
    
    try:
        # Load file as images
        images, source_desc = load_file_as_images(file_path)
        
        if not images:
            return [{"error": f"Could not load {category} file as images"}]
        
        # Define VLM extraction pipeline
        extraction_methods = [
            ("gemini", extract_with_gemini_flash),
            ("openai", extract_with_openai_gpt4o_mini),
            ("claude", extract_with_claude_haiku)
        ]
        
        # Force specific method if requested
        if force_method:
            extraction_methods = [(m, f) for m, f in extraction_methods if m == force_method]
        
        last_error = None
        
        for method_name, extraction_func in extraction_methods:
            try:
                # Check API key
                key_map = {
                    "gemini": ["GEMINI_API_KEY", "GOOGLE_API_KEY"],
                    "openai": ["OPENAI_API_KEY"],
                    "claude": ["ANTHROPIC_API_KEY"]
                }
                
                has_key = any(os.getenv(k) for k in key_map[method_name])
                
                if not has_key:
                    print(f"‚è≠Ô∏è Skipping {method_name.upper()}: API key not found")
                    continue
                
                # Attempt extraction
                result = extraction_func(images, source_desc)
                
                providers = result.get("providers", [])
                
                if not providers:
                    print(f"‚ö†Ô∏è {method_name.upper()}: No providers extracted, trying next method...")
                    continue
                
                # Validate and standardize
                validated_providers = []
                for provider in providers:
                    validated_provider = {
                        "full_name": provider.get("full_name") or provider.get("fullName", ""),
                        "NPI": provider.get("NPI") or provider.get("npi", ""),
                        "specialty": provider.get("specialty", ""),
                        "address": provider.get("address", ""),
                        "city": provider.get("city", ""),
                        "state": provider.get("state", ""),
                        "zip_code": provider.get("zip_code") or provider.get("zipCode", ""),
                        "phone": provider.get("phone", ""),
                        "license_number": provider.get("license_number") or provider.get("license", ""),
                        "website": provider.get("website", ""),
                        "last_updated": provider.get("last_updated") or provider.get("lastUpdated", datetime.now().strftime("%Y-%m-%d"))
                    }
                    
                    if validated_provider["full_name"]:
                        validated_providers.append(validated_provider)
                
                if not validated_providers:
                    print(f"‚ö†Ô∏è {method_name.upper()}: No valid providers after validation")
                    continue
                
                # SUCCESS!
                print(f"\n{'='*60}")
                print(f"‚úÖ VLM EXTRACTION SUCCESSFUL")
                print(f"{'='*60}")
                print(f"Source: {source_desc}")
                print(f"Method: {result.get('extraction_method', method_name).upper()}")
                print(f"Providers: {len(validated_providers)}")
                print(f"Confidence: {result.get('extraction_confidence', 0)*100:.1f}%")
                print(f"Pages: {result.get('pages_processed', len(images))}")
                print(f"{'='*60}\n")
                
                return validated_providers
                
            except Exception as e:
                last_error = str(e)
                print(f"‚ùå {method_name.upper()} failed: {last_error}")
                print(f"   Attempting next method...\n")
                continue
        
        # All methods failed
        error_msg = f"All extraction methods failed. Last error: {last_error}"
        print(f"\n{'='*60}")
        print(f"‚ùå EXTRACTION FAILED")
        print(f"{'='*60}")
        print(error_msg)
        print(f"{'='*60}\n")
        
        return [{"error": error_msg}]
        
    except Exception as e:
        error_msg = f"File loading failed: {str(e)}"
        print(f"\n{'='*60}")
        print(f"‚ùå FILE LOADING FAILED")
        print(f"{'='*60}")
        print(error_msg)
        print(f"{'='*60}\n")
        
        return [{"error": error_msg}]


# ============================================
# BACKWARD COMPATIBILITY ALIASES
# ============================================

# Keep old function name for existing code
parse_provider_pdf = parse_provider_file


# ============================================
# BATCH PROCESSING
# ============================================

def batch_parse_provider_files(file_paths: List[str]) -> Dict[str, List[Dict[str, Any]]]:
    """
    Process multiple files at once.
    
    Returns:
        Dictionary mapping filename -> list of providers
    """
    print(f"\n{'='*60}")
    print(f"üìö BATCH PROCESSING {len(file_paths)} FILES")
    print(f"{'='*60}\n")
    
    results = {}
    
    for i, file_path in enumerate(file_paths, 1):
        print(f"\n[{i}/{len(file_paths)}] Processing: {os.path.basename(file_path)}")
        print("-" * 60)
        
        providers = parse_provider_file(file_path)
        results[os.path.basename(file_path)] = providers
        
        # Summary
        if providers and not providers[0].get("error"):
            print(f"‚úÖ Success: {len(providers)} provider(s)")
        else:
            print(f"‚ùå Failed: {providers[0].get('error', 'Unknown error')}")
    
    print(f"\n{'='*60}")
    print(f"üìä BATCH PROCESSING COMPLETE")
    print(f"{'='*60}")
    
    total_providers = sum(len(p) for p in results.values() if p and not p[0].get("error"))
    print(f"Total providers extracted: {total_providers}")
    print(f"Files processed: {len(file_paths)}")
    print(f"{'='*60}\n")
    
    return results


# ============================================
# TEST CASES
# ============================================

if __name__ == '__main__':
    load_dotenv()
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

    print("="*60)
    print("RUNNING COMPREHENSIVE TESTS")
    print("="*60)
    
    # Test 1: NPI Search
    print("\n--- Test 1: Search by NPI ---")
    print(json.dumps(search_npi_registry(npi_number="1235256050"), indent=2))
    
    # Test 2: PDF Parsing
    print("\n--- Test 2: Parse Provider PDF ---")
    pdf_path = os.path.join(SCRIPT_DIR, 'Sample_Data', 'provider_directory.pdf')
    if os.path.exists(pdf_path):
        providers = parse_provider_file(pdf_path)
        if providers and not providers[0].get("error"):
            print(f"\n‚úÖ Extracted {len(providers)} provider(s)")
            print(f"\nFirst provider:")
            print(json.dumps(providers[0], indent=2))
    else:
        print(f"‚ö†Ô∏è PDF not found: {pdf_path}")
    
    # Test 3: Excel Parsing
    print("\n--- Test 3: Parse Excel File ---")
    excel_path = os.path.join(SCRIPT_DIR, 'Sample_Data', 'providers.xlsx')
    if os.path.exists(excel_path):
        providers = parse_provider_file(excel_path)
        if providers and not providers[0].get("error"):
            print(f"\n‚úÖ Extracted {len(providers)} provider(s)")
    else:
        print(f"‚ö†Ô∏è Excel not found: {excel_path}")
    
    # Test 4: CSV Parsing
    print("\n--- Test 4: Parse CSV File ---")
    csv_path = os.path.join(SCRIPT_DIR, 'Sample_Data', 'test_small.csv')
    if os.path.exists(csv_path):
        providers = parse_provider_file(csv_path)
        if providers and not providers[0].get("error"):
            print(f"\n‚úÖ Extracted {len(providers)} provider(s)")
    else:
        print(f"‚ö†Ô∏è CSV not found: {csv_path}")
    
    # Test 5: Image Parsing
    print("\n--- Test 5: Parse Image File ---")
    img_path = os.path.join(SCRIPT_DIR, 'Sample_Data', 'test_small.jpg')
    if os.path.exists(img_path):
        providers = parse_provider_file(img_path)
        if providers and not providers[0].get("error"):
            print(f"\n‚úÖ Extracted {len(providers)} provider(s)")
    else:
        print(f"‚ö†Ô∏è Image not found: {img_path}")
    
    # Test 6: Web Scraping
    print("\n--- Test 6: Scrape Provider Website ---")
    test_url = "https://my.clevelandclinic.org/staff/9953-robert-ackerman"
    scraped_text = scrape_provider_website(test_url)
    print(f"Scraped {len(scraped_text)} characters")
    
    # Test 7: Address Validation
    print("\n--- Test 7: Validate Address ---")
    result = validate_address(
        address="1600 Amphitheatre Parkway",
        city="Mountain View",
        state="CA",
        zip_code="94043"
    )
    print(json.dumps(result, indent=2))
    
    print("\n" + "="*60)
    print("ALL TESTS COMPLETED")
    print("="*60)